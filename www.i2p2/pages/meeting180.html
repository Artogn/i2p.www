{% extends "_layout.html" %}
{% block title %}I2P Development Meeting 180{% endblock %}
{% block content %}<h3>I2P dev meeting, May 16, 2006</h3>
<div class="irclog">
<p>&lt; cervantes&gt;  moo: http://dev.i2p.net/pipermail/i2p/2006-May/001289.html
<p>&lt; cervantes&gt;  0) hi
<p>&lt; cervantes&gt;  1) jrandom's not here
<p>&lt; cervantes&gt;  2) ???
<p>&lt; cervantes&gt;  0) hi
<p>&lt; cervantes&gt;  hi
<p>&lt; cervantes&gt;  moving on to 1)
<p>&lt; cervantes&gt;  jrandom isn't here today, but he'll give us a status update tomorrow
<p>&lt; cervantes&gt;  2) ???
<p>&lt; cervantes&gt;  does anyone have anything else to add to the meeting?
<p>&lt; bar&gt;  i have a question
<p>&lt; cervantes&gt;  in that case...
<p>* cervantes winds up
<p>* cervantes stops winding
<p>&lt; Complication&gt;  Aha, a question...
<p>&lt; bar&gt;  the PRNG fix in cvs, will that improve the general performace or is it related to something else?
<p>&lt; cervantes&gt;  it's uncertain what consequences it might have in general
<p>&lt; Complication&gt;  I'm personally not aware of its total impact, but it does involve at least two behaviours I'm aware of:
<p>&lt; cervantes&gt;  but it specifically fixes a symptom with i2ptunnel
<p>* cervantes lets complication decomplicate 
<p>&lt; Complication&gt;  tunnel length randomization and IRC server choice (more generically, random selection from a list of I2PTunnel destinations)
<p>&lt; Complication&gt;  Tunnel length randomization probably has a significant effect on overall network health, since it allows clients who are permitted to compromise on tunnel length to actually do that
<p>&lt; Complication&gt;  So they won't be holding breath and building 2-hop tunnels, but also try some 1-hop tunnels
<p>&lt; Complication&gt;  (which on hard times, are much easier to get)
<p>&lt; cervantes&gt;  also irc connectivity might improve once it's  rolled out. Basically freshcoffee was never getting any client connections because it was second in the list - so with the next release the load should be evenly distributed between both servers
<p>&lt; bar&gt;  so the bug made people always go for the longer tunnel lengths if available?
<p>&lt; Complication&gt;  If I understood right, every randomization with smallish integers (e.g. pick 0 or 1) was affected
<p>&lt; Complication&gt;  I *think* randomizations with bigger integers (e.g. pick an integer between 0 and 100) were less affected
<p>&lt; Complication&gt;  if you're interested, you should probably ask jranom for details when he's back
<p>&lt; Complication&gt;  I may get the details wrong.
<p>&lt; bar&gt;  i see, thanks. good catch
<p>&lt; Complication&gt;  well, cervantes came here and started complaining about not getting any overload ;P
<p>&lt; cervantes&gt;  that was my understanding of it too
<p>&lt; cervantes&gt;  see...you don't get anything in life if you don't grumble :)
<p>&lt; cervantes&gt;  do any folk have other questions or topics for the meeting?
<p>&lt; fox&gt;  &lt; duck&gt;  yes
<p>&lt; Pi&gt;  a question about general net health : i see more and more clients get left behind i2p-version wise (2 still using 0.6.1.11 and so on). won't these clients make monitoring effects of changes to the core more and more harder? (as "fewer" seem to want to update)
<p>&lt; fox&gt;  * duck repeats above
<p>* w423412323 suggests a topic change along that line. ;)
<p>&lt; fox&gt;  &lt; duck&gt;  I was wondering, I have seen some funky tuning commits on the cvs mailinglist. are those more experiments? are they based on observations? are they premature?
<p>&lt; Complication&gt;  Pi: as long as they aren't present in big numbers, they shouldn't make a big difference
<p>&lt; Pi&gt;  70 of 300 clients using non-0.6.1.18-version according to my netdb now 
<p>&lt; Complication&gt;  It's a game of numbers and capacity - if either most routers, or additionally the highest-capacity routers are reasonably updated, some people forgetting that they installed I2P shouldn't matter :)
<p>&lt; cervantes&gt;  Pi: if the older routers misbehave then the network _should_ adapt and reduce traffice being router via them
<p>&lt; cervantes&gt;  *being routed
<p>&lt; cervantes&gt;  Complication: did you see duck's question? 
<p>&lt; Pi&gt;  and a question about a stat on the i2p-console which appeared some time ago : what does handle backlog mean?
<p>&lt; Complication&gt;  duck: would you mean the tunnel throttle adjustments? They're tuning in the sense that they won't bring much inherently new, but they should be fairly well-tested now (e.g. they probably won't byte)
<p>&lt; Complication&gt;  But they might byte a little, if you run an exotic setup which is completely outside the parameters I could think of
<p>&lt; fox&gt;  &lt; duck&gt;  Complication: I was wondering if '2' instead of '3' thingies really mattered that much
<p>&lt; fox&gt;  &lt; duck&gt;  but it seemed that the random problem might have been a big baddy
<p>&lt; fox&gt;  &lt; duck&gt;  (though the relative impact of that towards network unhealth depends on when it was introduced)
<p>&lt; cervantes&gt;  Pi: handle backlog is the number of pending inbound tunnel join requests (quoted from the changelog)
<p>&lt; Complication&gt;  If you mean the random nextInteger() problem, and effect on tunnel length randomization, I feel it would have significant effect
<p>&lt; Complication&gt;  The cost difference of building a 1-hop and 2-hop tunnel is pretty significant
<p>&lt; Pi&gt;  thx, cervantes :)
<p>&lt; fox&gt;  &lt; duck&gt;  when was it introduced?
<p>&lt; Complication&gt;  duck: I think it was introduced with some switchovers to the Fortuna generator, or some modification therein
<p>&lt; fox&gt;  &lt; duck&gt;  ok; thanks a lot for your input
<p>&lt; Complication&gt;  Let me check the cvsweb for more detail...
<p>&lt; cervantes&gt;  Pi: I believe there's code in place now that drops inbound tunnel requests if the queue fills up (to help reduce cpu load)
<p>&lt; Complication&gt;  Pi: yes, that should be the visible indicator of another parameter used for deciding "do we have enough capacity to participate in another tunnel?"
<p>&lt; cervantes&gt;  duck: I certainly experience a large change in router behaviour since the fix was introduced. - not all good I have to say :)
<p>&lt; Complication&gt;  big handle backlog == congestion, no point in trying to join other people's tunnels
<p>&lt; cervantes&gt;  had a load average of 14 and 12000 participating tunnels the other day
<p>&lt; Complication&gt;  Handle backlog seems important particularly on high-capacity routers (referring to what cervantes saw)
<p>&lt; Complication&gt;  Low capacity routers generally throttle their tunnel acceptance for bandwidth reasons
<p>&lt; Complication&gt;  (or tunnel test time reasons, to be correct)
<p>&lt; Complication&gt;  (or at least, to try that)
<p>&lt; cervantes&gt;  wow we've managed half an hour....
<p>&lt; Complication&gt;  Indeed :D
<p>&lt; cervantes&gt;  anyone want to bring anything else to the table?
<p>&lt; cervantes&gt;  in that case...
<p>* cervantes winds up
<p>* cervantes *baffs* the meeting closed
<p>&lt; fox&gt;  &lt; duck&gt;  thx for taking care of the meeting
<p>&lt; cervantes&gt;  heh I was expecting to baf it closed before anything said anything....but bar ruined that plan :)
</div>
{% endblock %}